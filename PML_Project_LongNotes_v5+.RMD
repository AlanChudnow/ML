---
title: "Machine Learning Project Notes going forward V5+"
author: "Chudnow"
date: "June 13, 2015"
output: word_document
---

## 00. Checklist

   [] R Markdown File on GITHUB
      [] < 2000 words
      [] < 5 Figures
      [] Address out of sample error and address via cross-validation
   [] HTML File on GITHUB
   [] Submit Prediction Files for Automatic Grading
   
   https://github.com/AlanChudnow/DP/blob/gh-pages/ShinyProject_Slides.rmd.html
   http://AlanChudnow.github.io/DP/ShinyProject_Slides.rmd.html   



## 0. Background

### Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 


### Weightlifting Dataset from http://groupware.les.inf.puc-rio.br/har

This human activity recognition research has traditionally focused on discriminating between different activities, i.e. to predict "which" activity was performed at a specific point in time (like with the Daily Living Activities dataset above). The approach we propose for the Weight Lifting Exercises dataset is to investigate "how (well)" an activity was performed by the wearer. The "how (well)" investigation has only received little attention so far, even though it potentially provides useful information for a large variety of applications,such as sports training.

In this work (see the paper) we first define quality of execution and investigate three aspects that pertain to qualitative activity recognition: the problem of specifying correct execution, the automatic and robust detection of execution mistakes, and how to provide feedback on the quality of execution to the user. We tried out an on-body sensing approach (dataset here), but also an "ambient sensing approach" (by using Microsoft Kinect - dataset still unavailable) 

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).

Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience. We made sure that all participants could easily simulate the mistakes in a safe and controlled manner by using a relatively light dumbbell (1.25kg).

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. 

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3d4K34xch


### Data 

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment. 

### What you should submit

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

1. Your submission should consist of a link to a Github repo with your
** R markdown**  and **compiled HTML file** describing your analysis. Please constrain the text of the **writeup to < 2000 words** and the **number of figures to be less than 5.** It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).

2. You should also *apply your machine learning algorithm to the 20 test cases available in the test data above*. Please *submit your predictions in appropriate format to the programming assignment for automated grading*. See the programming assignment for additional details. 



## Reproducibility 

Due to security concerns with the exchange of R code, your code will not be run during the evaluation by your classmates. Please be sure that if they download the repo, they will be able to view the compiled HTML version of your analysis. 

## 1. Read in the Data 

```{r}
library(caret)

SeedVersion <- 531

fn_Train <- "pml-training.csv"
fn_Test <- "pml-testing.csv"

df_rawTrain <-read.csv(fn_Train)  #Raw data directly from file
df_rawTest <- read.csv(fn_Test)   #Raw test data directly from file

```


## 2. Preliminary Exploration and Data Cleaning prior to Training

An examination of the data using R and Excel indicated that the training set had a number of factors that I didn't need to carry around for testing and training. Examples include:

* Col 1 Index (not relevant)
* Col 2 user_name (not relevant)
* Col 3:7 time and window number (not relevant)
* Col 12 kurosis (Almost all blank)
* Col 18 max_roll (almost all NA)

```{r}
library(caret)

#Can I get rid of any columns first that don't change much
nzv <- nearZeroVar(df_rawTrain,saveMetrics=TRUE)
dropCol <- nzv$nzv

#Get rid of the first 7 columns are just indexes
dropCol[1:7] <- TRUE

#Identify and get rid of columns that are almost all NAs
countNA <- apply(df_rawTrain,2,function(x) {sum(is.na(x))})
dropNA <- countNA>(0.9*dim(df_rawTrain)[1])

dropCol[dropNA] <- TRUE

cleanPivot <- function(bigdf,dropCol){
    #This function will drop all the cols in my table
    #It will move the last col to be the first column
    sdf0 <- bigdf[,dropCol==FALSE]
    last <- length(sdf0)
    sdf1 <- data.frame(classe=sdf0[,last], sdf0[,1:(last-1)])
    return(sdf1)
}

df_train0 <- cleanPivot(df_rawTrain,dropCol) #Cleaned Training Set
df_TEST0 <- cleanPivot(df_rawTest, dropCol)  #Cleaned Final Test Set

```

## 3 Create Training/Test and Quiz Set for Cross-Validation

To design the prediction study, data is split into three sets

1. 60% Training - To provide data for machine learning (ML) algorithms 
2. 20% Test set - To probe / test specific ML algorithms and settings
3. 20% Quiz set - To validate algorithms after selection

This worksheet is set up so that training and test sets can be randomly reassigned by changing a global `SeedVersion` value in the first block. This allows me to repeat the experiment with a different shuffle of rows into test and training sets, but leaving the Quiz set as is.

```{r}

set.seed(135)
inTrain <- createDataPartition(df_train0[,1], p = 0.8)[[1]]

#Partion Data into BigTrain is for Training; quizing is for Validation
BigTrain   <- df_train0[ inTrain,];   quizing <- df_train0[-inTrain,]
BigTrain_u <- df_rawTrain[inTrain,2]; quiz_u  <- df_rawTrain[-inTrain,2]
BigTrain_c <- BigTrain[,1];           quiz_c  <- quizing[,1]

#Partion BigTrain into Training and Testing data as described above
set.seed(SeedVersion)
inTrain <- createDataPartition(BigTrain_c, p = 0.75)[[1]]

training <-BigTrain[ inTrain,];   testing <- BigTrain[-inTrain,]
train_u <- BigTrain_u[inTrain];   test_u <- BigTrain_u[-inTrain]
train_c <- training[,1];          test_c <- testing[,1]

colmax <- dim(training)[2]
colnames(training)

```


## 4 Examine Training Data for any obvious features that can be exploited

### 4a. Plot the data for each dimension vs. column and color by class

```{r}
library(manipulate)

myPlot <- function(cnum) {
    plot(1:length(training[,cnum]),training[,cnum],
         col=(as.numeric(training[,1])+1),
         ylab=colnames(training)[cnum],
         main=colnames(training)[cnum])
}

myBox <- function(cnum) {
    plot(training[,cnum] ~ training[,1], 
         col=(as.numeric(training[,1])+1),
         ylab=colnames(training)[cnum],
         main=colnames(training)[cnum])
}

```
Note: These commands will allow for the user to graph each index.  Just cut/paste into console window (after running hte R code above

manipulate(myPlot(cnum),cnum=slider(2,dim(training)[2],step=1))
manipulate(myBox(cnum),cnum=slider(2,dim(training)[2],step=1))

Note: There are no obvious single columns that correlate well with states
I have several rows that have clear outliers: gyros_dumbbell_x

### 4b. Explore by seeing the Two dimensional SVD

It turns out that using an 2-D PCA analysis will form users into clusters quite readily but the relationship between different classes is not so obvious.


```{r}
set.seed(SeedVersion)
preProc_pca2 <- preProcess(training[,-1], method="pca", pcaComp=2)
    #preProc_pca2
    #preProc_pca2$rotation

trainP_pca2 <- predict(preProc_pca2, training[,-1])
    plot(trainP_pca2$PC1,trainP_pca2$PC2, col=(as.numeric(train_u)+1),
         main="Users easily cluster in 2D PCA",
         xlab="PC1", ylab="PC2" )
    
    plot(trainP_pca2$PC1,trainP_pca2$PC2, col=(as.numeric(train_c)+1),
         main="classe vs 2D PCA shows significant overlap",
         xlab="PC1", ylab="PC2" )



```

### 4c. Three dimensional PCA, just for fun

With a 3D PCA, there does appear to finally be a clustering of classe that we can take exploit (Although I did not code an algorithm to do this)

```{r}
library(plot3D)
    
set.seed(SeedVersion)
preProc_pca3 <- preProcess(training[,-1], method="pca", pcaComp=3)
    preProc_pca3
    preProc_pca3$rotation

trainP_pca3 <- predict(preProc_pca3, training[,-1])
    plot(trainP_pca3$PC1,trainP_pca3$PC2, col=(as.numeric(test_c)+1))
    plot(trainP_pca3$PC2,trainP_pca3$PC3, col=(as.numeric(test_c)+1))
    plot(trainP_pca3$PC3,trainP_pca3$PC1, col=(as.numeric(test_c)+1))
    scatter3D(trainP_pca3$PC1,trainP_pca3$PC2, trainP_pca3$PC3,
              xlim=c(-8,8), ylim=c(-5,7),zlim=c(-4,8),clim=c(1,6),
              main="3D PCA shows clustering by classe (color)",
           col=(as.numeric(test_c)+1),phi=0,theta=10)
    
    
my3D<- function(v_phi,v_theta) {
    scatter3D(trainP_pca3$PC1,
              trainP_pca3$PC2, 
              trainP_pca3$PC3,
              xlim=c(-8,8), ylim=c(-6,7),zlim=c(-4,8),clim=c(1,6),
              xlab="PC1",ylab="PC2",zlab="PC3",
              phi=v_phi,
              theta=v_theta,
              col=(as.numeric(train_c)+1))
}
```

manipulate(my3D(v_phi,v_theta),
           v_phi=slider(-90,90,step=5,initial=0),
           v_theta=slider(-90,90,step=5,initial=10)
           )


## 5. Using Machine learning models to predict classe

## 5A. KNN Techniques (Zero Mean/ 1 sigma) *Accuracy : 0.98* 

kNN has many useful characteristics, one of which being its insensitivity to outliers that makes it resilient to any errors in the classification data (the supervised learning phase). As a downside, the algorithm is noted for its CPU and memory greediness.  For this example we scale all the columns for mean 0 and sd=1 prior to running the algorithm.

```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1105    4    5    2    0
         B   14  729   14    1    1
         C    3    7  665    8    1
         D    0    1   12  627    3
         E    1    3    4    4  709

Overall Statistics:

               Accuracy : 0.9776         
                 95% CI : (0.9724, 0.982)
```                 

```{r}
library (class)

znorm <- function(x) { return ((x - mean(x)) / sd(x))}

x<- znorm(rnorm(10,mean=30,sd=5)); mean(x); sd(x)  #Check Code

train_knnZ <- as.data.frame(lapply(training[2:colmax],znorm))
test_knnZ <-  as.data.frame(lapply(testing[2:colmax],znorm))

#Loop over different values of K to see which works best

x <- c(1,2,3,4,5,6,7,8,9,10,15,25,30,40,60,80,100) #valued of K
y <- x * 0

set.seed(SeedVersion)
for(nNN in 1:length(x)){
    knn_test_pred <- knn(train = train_knnZ, #Training Set
                     test = test_knnZ,  #Test Data
                      cl = train_c, #Truth Labels for Training Data
                      k=x[nNN]) #Number of valued to compare
    cl_knn<- confusionMatrix(test_c,knn_test_pred)
    y[nNN] <- cl_knn$overall[1]
}

plot(x,y,xlab="No of Nearest Neighbors", 
     ylab="overall accuracy", 
     main="Effect of No of Nearest Neighbors (N(0,1) Scaling)")
y
kBest = x[which(y==max(y))]

#set kBest<-3 Anyway because 1 seems overtraining
kBest <-3

knn_test_pred <- knn(train = train_knnZ, test = test_knnZ, 
                     cl = train_c, k=kBest)

cl_knn<- confusionMatrix(test_c,knn_test_pred)
cl_knn
cl_knn$overall[1]
kBest


erroring <- testing[-(test_c==knn_test_pred),]
error_pca3 <- predict(preProc_pca3, erroring[,-1])

    scatter3D(trainP_pca3$PC1,trainP_pca3$PC2, trainP_pca3$PC3,
              xlim=c(-8,8), ylim=c(-5,7),zlim=c(-4,8),clim=c(1,6),
              main="3D PCA shows clustering by classe (color)",
           col=(as.numeric(test_c)+1),phi=0,theta=10)
    
    scatter3D(error_pca3$PC1,error_pca3$PC2, error_pca3$PC3,
              xlim=c(-8,8), ylim=c(-5,7),zlim=c(-4,8),clim=c(1,6),
              main="KNN Errors by classe (color)",
           col=(as.numeric(erroring[,1])+1),phi=0,theta=10)


    
flip3D<- function(v_phi,v_theta,flip) {
    if(flip==0){
            scatter3D(trainP_pca3$PC1,
              trainP_pca3$PC2, 
              trainP_pca3$PC3,
              xlim=c(-8,8), ylim=c(-6,7),zlim=c(-4,8),clim=c(1,6),
              main="DATA", xlab="PC1",ylab="PC2",zlab="PC3",
              phi=v_phi,
              theta=v_theta,
              col=(as.numeric(train_c)+1))
    }
    if(flip==1){
            scatter3D(error_pca3$PC1,
              error_pca3$PC2, 
              error_pca3$PC3,
              xlim=c(-8,8), ylim=c(-6,7),zlim=c(-4,8),clim=c(1,6),
              main="ERROR", xlab="PC1",ylab="PC2",zlab="PC3",
              phi=v_phi,
              theta=v_theta,
              col=(as.numeric(erroring[,1])+1))
        
    }

}
 
#manipulate(flip3D(v_phi,v_theta,flip),
#           v_phi=slider(-90,90,step=5,initial=0),
#           v_theta=slider(-90,90,step=5,initial=10),
#           flip=slider(0,1,step=1,initial=0)
#           )
 
#try it on the QUIZ SET - But Don't Look Yet
Quiz_knnZ <- as.data.frame(lapply(quizing[2:colmax],znorm))
knnN0_Quiz_pred <- knn(train = train_knnZ, #Training Set
                     test = Quiz_knnZ,  #Test Data
                     cl = train_c, #Truth Labels for Training Data
                     k=kBest) #Number of valued to compare
knn_cl_quiz <- confusionMatrix(quiz_c, knnN0_Quiz_pred)

#Try it on the FINAL TEST SET - And Save Answers for later output
TEST0_knnZ <- as.data.frame(lapply(df_TEST0[2:colmax],znorm))
knnN0_TEST0_pred <- knn(train = train_knnZ, #Training Set
                     test = TEST0_knnZ,  #Test Data
                     cl = train_c, #Truth Labels for Training Data
                     k=kBest) #Number of valued to compare


```

## 5b.  KNN Techniques (Min/Max Scaling)  *Accuracy : 0.94%*

For this example we scale all the columns for mean 0 and sd=1 prior to running the algorithm. Because I have outliers, this may do worse than the approach N(0,1) above because the outliers will squeeze the data. 

```
Reference
Prediction    A    B    C    D    E
         A 1091   18    3    4    0
         B   22  707   24    1    5
         C    3   26  608   37   10
         D    1    3   25  610    4
         E    0    4   11   10  696

Overall Statistics
                                          
               Accuracy : 0.9462          
                 95% CI : (0.9387, 0.9531)
```

```{r}
library (class)

normalize <- function(x) { return ((x - min(x)) / (max(x) - min(x)))}

normalize(c(1, 2, 3, 4, 5))  #Check Code
normalize(c(10, 20, 30, 40, 50)) #Check Code

#Create a set of scaled test/training sets for the algorithm#
#In this case all data is between scaled to between 0 and 1

train_knnMM <- as.data.frame(lapply(training[2:colmax],normalize))
test_knnMM <-  as.data.frame(lapply(testing[2:colmax],normalize))
#remember train_c <- training[,1]; #Labels for training
#remember test_c <- testing[,1] #Labels for testing

#Try this at a number of different nearest neighbors and pick the best
x <- c(1,2,5,10,15,20,25,30,40,60,80,100)
y <- x * 0

set.seed(SeedVersion)
for(nNN in 1:length(x)){
    knn_test_pred <- knn(train = train_knnMM, #Training Set
                     test = test_knnMM,  #Test Data
                      cl = train_c, #Truth Labels for Training Data
                      k=x[nNN]) #Number of valued to compare

    cl_knn<- confusionMatrix(test_c,knn_test_pred)
    y[nNN] <- cl_knn$overall[1]
}

plot(x,y,xlab="No of nearest neighbors compared",
     ylab="overall accuracy",
     main="Effect of No of Nearest Neighbors (Min/Max Scaling)")
y

kBest = x[which(y==max(y))]
knn_test_pred <- knn(train = train_knnMM, #Training Set
                     test = test_knnMM,  #Test Data
                      cl = train_c, #Truth Labels for Training Data
                      k=kBest) #Number of valued to compare


#try it on the QUIZ SET But Don't Look Yet
Quiz_knnMM <- as.data.frame(lapply(quizing[2:colmax],znorm))
knnMM_Quiz_pred <- knn(train = train_knnMM, #Training Set
                     test = Quiz_knnMM,  #Test Data
                     cl = train_c, #Truth Labels for Training Data
                     k=kBest) #Number of valued to compare

#try it on the TEST SET But Don't Look Yet.
TEST0_knnMM <- as.data.frame(lapply(df_TEST0[2:colmax],normalize))
knnMM_TEST0_pred <- knn(train = train_knnMM, #Training Set
                     test = TEST0_knnMM,  #Test Data
                     cl = train_c, #Truth Labels for Training Data
                     k=kBest) #Number of valued to compare

cl_knn<- confusionMatrix(test_c,knn_test_pred)
cl_knn
cl_knn$overall[1]
kBest

```

## 5c. NaiveBayes *Accuracy : 49%*

This algorithms computes the conditional a-posterior probabilities of a categorical class variable given independent predictor variables using the Bayes rule. Bayes performed especially poorly on this example, although better than random guessing.

```
Confusion Matrix and Statistics

          Reference
Prediction   A   B   C   D   E
         A 319  93 494 179  31
         B  19 456 175  54  55
         C   7  54 484 104  35
         D   0  18 235 317  73
         E  12 150 113  98 348

Overall Statistics
                                          
               Accuracy : 0.4904          
                 95% CI : (0.4747, 0.5062)
    No Information Rate : 0.3826       
```    

```{r}

require(e1071)

set.seed(SeedVersion)
m_fitBayes <- naiveBayes(classe ~ ., data=training, laplace = 0)
bayes_test_pred <- predict(m_fitBayes,testing,type="class" ) 
cl_bayes<- confusionMatrix(test_c,bayes_test_pred)
cl_bayes$overall[1]

bayes_Quiz_pred <- predict(m_fitBayes,quizing,type="class")
bayes_TEST0_pred <- predict(m_fitBayes,df_TEST0,type="class" )

```

## 5d. Logistical Regression *Accuracy 74%*

Logistic Regression is a classification method that models the probability of an observation belonging to one of two classes. As such, normally logistic regression is demonstrated with binary classification problem (2 classes). Logistic Regression can also be used on problems with more than two classes (multinomial), as in this case.

See http://machinelearningmastery.com/linear-classification-in-r/

```
Confusion Matrix and Statistics

          Reference
Prediction   A   B   C   D   E
         A 965  33  62  47   9
         B  97 501  79  15  67
         C  66  62 479  45  32
         D  32  27  68 486  30
         E  31 105  42  68 475

Overall Statistics
                                          
               Accuracy : 0.7408          
                 95% CI : (0.7267, 0.7544)
```                 

```{r}

library(VGAM)

set.seed(SeedVersion)
fit_vgam <- vglm(classe~., family=multinomial, data=training)

#summary(fit_vgam)

prob_vgam <- predict(fit_vgam, testing, type="response")
pred_vgam <- apply(prob_vgam, 1, which.max)
pred_vgam[which(pred_vgam=="1")] <- levels(testing$classe)[1]
pred_vgam[which(pred_vgam=="2")] <- levels(testing$classe)[2]
pred_vgam[which(pred_vgam=="3")] <- levels(testing$classe)[3]
pred_vgam[which(pred_vgam=="4")] <- levels(testing$classe)[4]
pred_vgam[which(pred_vgam=="5")] <- levels(testing$classe)[5]

cl_vgam<- confusionMatrix(test_c,pred_vgam)
cl_vgam
cl_vgam$overall[1]

pq_vgam <- predict(fit_vgam, quizing ,type="response")
vgam_quiz_pred <- apply(pq_vgam, 1, which.max)
vgam_quiz_pred[which(vgam_quiz_pred=="1")] <- levels(testing$classe)[1]
vgam_quiz_pred[which(vgam_quiz_pred=="2")] <- levels(testing$classe)[2]
vgam_quiz_pred[which(vgam_quiz_pred=="3")] <- levels(testing$classe)[3]
vgam_quiz_pred[which(vgam_quiz_pred=="4")] <- levels(testing$classe)[4]
vgam_quiz_pred[which(vgam_quiz_pred=="5")] <- levels(testing$classe)[5]

pTEST0_vgam <- predict(fit_vgam, df_TEST0 ,type="response")
vgam_TEST0_pred <- apply(pTEST0_vgam, 1, which.max)
vgam_TEST0_pred[which(vgam_TEST0_pred=="1")] <- levels(testing$classe)[1]
vgam_TEST0_pred[which(vgam_TEST0_pred=="2")] <- levels(testing$classe)[2]
vgam_TEST0_pred[which(vgam_TEST0_pred=="3")] <- levels(testing$classe)[3]
vgam_TEST0_pred[which(vgam_TEST0_pred=="4")] <- levels(testing$classe)[4]
vgam_TEST0_pred[which(vgam_TEST0_pred=="5")] <- levels(testing$classe)[5]

```


## 5e. Linear Discriminant Analysis *Accuracy 69%*

LDA is a classification method that finds a linear combination of data attributes that best separate the data into classes.

See http://machinelearningmastery.com/linear-classification-in-r/

```
Confusion Matrix and Statistics

          Reference
Prediction   A   B   C   D   E
         A 906  31  92  85   2
         B 126 467  99  24  43
         C  85  63 439  80  17
         D  28  25  66 497  27
         E  24 131  54  84 428

Overall Statistics
                                        
               Accuracy : 0.6977        
                 95% CI : (0.683, 0.712)
```

```{r}

library(MASS)

set.seed(SeedVersion)
fit_lda <- lda(classe~., data=training)
summary(fit_lda)
pred_lda <- predict(fit_lda, testing)$class
cl_lda<- confusionMatrix(test_c,pred_lda)
cl_lda
cl_lda$overall[1]

lda_quiz_pred <- predict(fit_lda,quizing)$class
lda_TEST0_pred <- predict(fit_lda, df_TEST0)$class

```


## 5f.  Partial Least Squares Discriminant Analysis *Accuracy 39%%*

Partial Least Squares Discriminate Analysis is the application of LDA on a dimension-reducing projection of the input data (partial least squares).

See http://machinelearningmastery.com/linear-classification-in-r/

Confusion Matrix and Statistics

```
Reference
Prediction   A   B   C   D   E
         A 587  89 233 176  31
         B 108 290 172 135  54
         C 153 118 287 104  22
         D  33  81 172 307  50
         E  96 185 180 169  91

Overall Statistics
                                          
               Accuracy : 0.3982          
                 95% CI : (0.3828, 0.4137)
```

```{r}

library(caret)

train_plsda <- training[,2:colmax]
test_plsda <- testing[,2:colmax]

set.seed(SeedVersion)
fit_plsda <- plsda(train_plsda,train_c, probMethod="Bayes")
pred_plsda <- predict(fit_plsda, test_plsda)
cl_plsda<- confusionMatrix(test_c,pred_plsda)
cl_plsda
cl_plsda$overall[1]

```

## 6. Validate the best algorithm on quiz data

The KNN Algorithm using N0,1) scaling proved to be the best algorithm.  Here it is running on the Quiz data: 

```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1108    7    1    0    0
         B   10  734   12    0    3
         C    0    5  670    9    0
         D    0    0   21  622    0
         E    3    4    4    2  708

Overall Statistics
                                          
               Accuracy : 0.9794          
                 95% CI : (0.9744, 0.9836)

```{r}

knn_cl_quiz <- confusionMatrix(quiz_c, knnN0_Quiz_pred)
knn_cl_quiz
```

## 6. Stack the Test Results from each model Just for fun and to compare

```{r}
stackResults = data.frame(
    knnN0 = knnN0_TEST0_pred,
    knnMM = knnMM_TEST0_pred,
    bayes = bayes_TEST0_pred,
    vgam = vgam_TEST0_pred,
    lda = lda_TEST0_pred
)
stackResults


```


## 7. Write Answers to File

```{r}

answers = knnN0_TEST0_pred

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
Sys.time()
```
